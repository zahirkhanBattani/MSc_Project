{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ed19d5-d25a-4e33-b87b-129a6c2782fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading data...\n",
      "‚úÖ Data prepared: (3200, 3297)\n",
      "‚ö†Ô∏è Thresholds file not found, using default quartiles\n",
      "\n",
      "‚è≥ Starting hyperparameter tuning (this may take 15-60 minutes depending on n_iter)...\n",
      "\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "============================================================\n",
      "üèÜ BEST PARAMETERS FOUND:\n",
      "============================================================\n",
      "  subsample           : 0.9\n",
      "  reg_lambda          : 0.1\n",
      "  reg_alpha           : 1\n",
      "  n_estimators        : 300\n",
      "  min_child_weight    : 7\n",
      "  max_depth           : 5\n",
      "  learning_rate       : 0.1\n",
      "  gamma               : 0\n",
      "  colsample_bytree    : 1.0\n",
      "============================================================\n",
      "\n",
      "üìä Best Cross-Validation F1 Score: 0.6241\n",
      "üìà Test Set F1 Score: 0.4482\n",
      "\n",
      "üíæ Saved fine-tuned model: ../notebooks/models/xgb_reg_finetuned.json\n",
      "üíæ Saved fine-tuned model (pkl): ../notebooks/models/xgb_reg_finetuned.pkl\n",
      "üíæ Saved best parameters: ../notebooks/models/best_params.json\n",
      "\n",
      "‚úÖ Hyperparameter tuning complete!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# XGBoost Regression Hyperparameter Tuning for F1 (Refined)\n",
    "# ==============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1) Copy helper functions from training notebook\n",
    "# ----------------------------------------------\n",
    "LMH_MAP = {'Low': 3, 'Medium': 5, 'High': 8}\n",
    "\n",
    "def normalize_inputs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Map 'Low/Medium/High' to numeric for key columns\n",
    "    - Strip thousand separators and coerce numerics for budget/count/time cols\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Columns that may appear as L/M/H but we need numeric for FE\n",
    "    lmh_cols = ['Integration_Complexity', 'Requirement_Stability', 'Market_Volatility']\n",
    "    for c in lmh_cols:\n",
    "        if c in df.columns:\n",
    "            if df[c].dtype == 'object':\n",
    "                s = df[c].astype(str).str.strip()\n",
    "                mapped = s.map(LMH_MAP)\n",
    "                numeric = pd.to_numeric(s.str.replace(',', ''), errors='coerce')\n",
    "                df[c] = mapped.fillna(numeric)\n",
    "\n",
    "    # Columns that should be numeric (may have commas)\n",
    "    numeric_cols = ['Project_Budget_USD', 'Team_Size', 'Estimated_Timeline_Months', 'Stakeholder_Count']\n",
    "    for c in numeric_cols:\n",
    "        if c in df.columns and df[c].dtype == 'object':\n",
    "            df[c] = pd.to_numeric(df[c].astype(str).str.replace(',', ''), errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if {'Project_Budget_USD','Team_Size'}.issubset(df.columns):\n",
    "        df['Budget_Per_TeamMember'] = df['Project_Budget_USD'] / (df['Team_Size'] + 1e-5)\n",
    "    if {'Estimated_Timeline_Months','Stakeholder_Count'}.issubset(df.columns):\n",
    "        df['Schedule_Pressure_Index'] = df['Estimated_Timeline_Months'] / (df['Stakeholder_Count'] + 1e-5)\n",
    "    if {'Integration_Complexity','Requirement_Stability','Market_Volatility'}.issubset(df.columns):\n",
    "        df['Complexity_Index'] = (\n",
    "            df['Integration_Complexity'].astype(float)\n",
    "            + df['Requirement_Stability'].astype(float)\n",
    "            + df['Market_Volatility'].astype(float)\n",
    "        ) / 3.0\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2) Load and prepare data (same as training)\n",
    "# ----------------------------------------------\n",
    "print(\"üìÇ Loading data...\")\n",
    "df = pd.read_csv(\"../data/project_risk_raw_dataset.csv\")\n",
    "\n",
    "# Normalize and engineer features\n",
    "df_clean = normalize_inputs(df)\n",
    "df_clean = feature_engineer(df_clean)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop(columns=[\"Risk_Level\"])\n",
    "y_labels = df_clean[\"Risk_Level\"]\n",
    "\n",
    "# Map labels to regression scores and class ints\n",
    "label_to_score = {'Low': 0.25, 'Medium': 0.50, 'High': 0.75, 'Critical': 1.00}\n",
    "label_to_class = {'Low': 0, 'Medium': 1, 'High': 2, 'Critical': 3}\n",
    "\n",
    "y_regression = y_labels.map(label_to_score)\n",
    "y_classification = y_labels.map(label_to_class)\n",
    "\n",
    "# Train/test split (stratify by original label)\n",
    "X_train_raw, X_test_raw, y_train_reg, y_test_reg, y_train_class, y_test_class = train_test_split(\n",
    "    X, y_regression, y_classification, test_size=0.2, random_state=42, stratify=y_labels\n",
    ")\n",
    "\n",
    "# One-hot encode (align test to train)\n",
    "X_train = pd.get_dummies(X_train_raw, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test_raw, drop_first=True)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(f\"‚úÖ Data prepared: {X_train.shape}\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 3) Load or compute global thresholds\n",
    "# ----------------------------------------------\n",
    "try:\n",
    "    with open(\"../notebooks/models/global_thresholds.json\", \"r\") as f:\n",
    "        T = json.load(f)\n",
    "    t25 = T[\"low_medium\"]\n",
    "    t50 = T[\"medium_high\"]\n",
    "    t75 = T[\"high_critical\"]\n",
    "    print(f\"‚úÖ Loaded thresholds: {t25:.3f}, {t50:.3f}, {t75:.3f}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Thresholds file not found, using default quartiles\")\n",
    "    t25, t50, t75 = 0.25, 0.50, 0.75\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 4) Helper: Convert regression scores ‚Üí classes\n",
    "# ----------------------------------------------\n",
    "def convert_to_class(arr_like):\n",
    "    \"\"\"Convert continuous array-like predictions to risk classes (0..3).\"\"\"\n",
    "    arr = np.asarray(arr_like, dtype=float)\n",
    "    # if 1D with shape (n,1) flatten\n",
    "    if arr.ndim > 1:\n",
    "        arr = arr.ravel()\n",
    "    labels = np.zeros_like(arr, dtype=int)\n",
    "    labels[arr >= t75] = 3\n",
    "    labels[(arr >= t50) & (arr < t75)] = 2\n",
    "    labels[(arr >= t25) & (arr < t50)] = 1\n",
    "    labels[arr < t25] = 0\n",
    "    return labels\n",
    "\n",
    "# Custom F1 scorer for regression model (robust)\n",
    "def f1_from_regression(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: could be floats (0.25,0.5,...) or ints (0..3) depending on how RandomizedSearchCV passes them.\n",
    "    y_pred: regression predictions (floats).\n",
    "    We'll convert both to class ints via thresholds before computing F1.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ensure numpy arrays & flatten\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        if y_pred.ndim > 1:\n",
    "            y_pred = y_pred.ravel()\n",
    "        # replace NaN preds with mid-value\n",
    "        y_pred = np.nan_to_num(y_pred, nan=(t25 + t50) / 2.0)\n",
    "\n",
    "        # Convert predictions to classes\n",
    "        y_pred_class = convert_to_class(y_pred)\n",
    "\n",
    "        # Convert truth to classes if needed\n",
    "        y_true = np.asarray(y_true)\n",
    "        # If truth values appear to be floats in label range, convert using thresholds\n",
    "        if np.issubdtype(y_true.dtype, np.floating):\n",
    "            y_true_class = convert_to_class(y_true)\n",
    "        else:\n",
    "            # try to coerce to int for safety\n",
    "            y_true_class = y_true.astype(int)\n",
    "\n",
    "        # compute F1 (weighted)\n",
    "        return f1_score(y_true_class, y_pred_class, average=\"weighted\")\n",
    "    except Exception as e:\n",
    "        # If anything goes wrong, return 0.0 (prevents CV from producing NaN)\n",
    "        # optionally you can print(e) for debugging but it floods logs in CV\n",
    "        return 0.0\n",
    "\n",
    "# wrap as sklearn scorer\n",
    "f1_scorer = make_scorer(f1_from_regression, greater_is_better=True)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5) Hyperparameter search space\n",
    "# ----------------------------------------------\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300, 500, 800, 1200],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 4, 5, 6, 8, 10],\n",
    "    \"subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"min_child_weight\": [1, 3, 5, 7],\n",
    "    \"gamma\": [0, 0.1, 0.3, 0.5, 1.0],\n",
    "    \"reg_alpha\": [0, 0.01, 0.1, 1],\n",
    "    \"reg_lambda\": [0.1, 1, 2, 5]\n",
    "}\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6) Randomized search with F1 optimization\n",
    "# ----------------------------------------------\n",
    "xgb_model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"    # safe & faster; change to \"gpu_hist\" if you have GPU\n",
    ")\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=40,  # Increase to 80-100 for better results if you have time\n",
    "    scoring=f1_scorer,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚è≥ Starting hyperparameter tuning (this may take 15-60 minutes depending on n_iter)...\\n\")\n",
    "# IMPORTANT: train with regression targets (continuous); scorer converts truths inside\n",
    "search.fit(X_train, y_train_reg)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 7) Results and save best model\n",
    "# ----------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ BEST PARAMETERS FOUND:\")\n",
    "print(\"=\"*60)\n",
    "for param, value in search.best_params_.items():\n",
    "    print(f\"  {param:20s}: {value}\")\n",
    "print(\"=\"*60)\n",
    "# Note: search.best_score_ is F1 computed by our scorer (should be finite now)\n",
    "print(f\"\\nüìä Best Cross-Validation F1 Score: {search.best_score_:.4f}\")\n",
    "\n",
    "# Get best model and evaluate on test set\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# Predict on test set (regression predictions)\n",
    "y_pred_reg = best_model.predict(X_test)\n",
    "# Convert to classes using thresholds\n",
    "y_pred_class = convert_to_class(y_pred_reg)\n",
    "\n",
    "# Calculate test set F1 using the integer test labels we saved earlier\n",
    "test_f1 = f1_score(y_test_class, y_pred_class, average=\"weighted\")\n",
    "print(f\"üìà Test Set F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 8) Save the tuned model and params\n",
    "# ----------------------------------------------\n",
    "MODEL_DIR = \"../notebooks/models\"\n",
    "\n",
    "# Save as JSON (most stable)\n",
    "best_model.save_model(f\"{MODEL_DIR}/xgb_reg_finetuned.json\")\n",
    "print(f\"\\nüíæ Saved fine-tuned model: {MODEL_DIR}/xgb_reg_finetuned.json\")\n",
    "\n",
    "# Also save as pickle for compatibility\n",
    "joblib.dump(best_model, f\"{MODEL_DIR}/xgb_reg_finetuned.pkl\")\n",
    "print(f\"üíæ Saved fine-tuned model (pkl): {MODEL_DIR}/xgb_reg_finetuned.pkl\")\n",
    "\n",
    "# Save best parameters\n",
    "with open(f\"{MODEL_DIR}/best_params.json\", \"w\") as f:\n",
    "    json.dump(search.best_params_, f, indent=2)\n",
    "print(f\"üíæ Saved best parameters: {MODEL_DIR}/best_params.json\")\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438eb94d-6962-4a97-bfa3-11f3adb4d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d524a-ed35-4654-9456-5bae6209b42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
